MVP Progress Report

ML for Grid Storage (Lucent v1)

Jacob B, Nathan C, Shamith L & Joshua L
University of Wollongong

===============================================================================

Executive Summary

This report details the MVP for our gravity-based energy storage mineshaft evaluation application. It outlines each component of the system, explains how they work together, and provides the current status in meeting our minimum viable product requirements.

The application consists of three core layers: a data ingestion layer that consolidates and standardizes information from multiple sources, a machine learning layer that evaluates mineshafts across key criteria, and a view layer that presents ranked recommendations through a web interface. This integrated pipeline enables data-driven selection of optimal sites for gravity energy storage infrastructure.

===============================================================================

Table of Contents

1. Introduction
2. Target Market
3. Design Methodology
4. Development Environment
5. Deployment Environment
6. ETL Pipeline (MVP)
7. Tools
8. Requirements Analysis
9. Data Classification Framework
10. Data Integration Plan
11. Design
12. Conclusion
References
Glossary

===============================================================================

1. Introduction

Green Gravity uses gravity batteries to transform renewable energy storage by storing energy through gravitational potential in decommissioned mineshafts. The transition from fossil fuels to renewable energy makes it essential to find solutions for storing intermittent energy sources such as solar and wind power. The project advances this mission by implementing machine learning to automate the mine site selection process. The large number of over 80,000 abandoned mine sites in Australia makes manual evaluation impossible. Our system analyses both structured and unstructured datasets to select high-potential sites using key metrics such as shaft depth and geographic location while considering proximity to infrastructure and environmental compatibility.

===============================================================================

2. Target Market

Green Gravity's internal stakeholders and external partners who stand to gain from our automated solution form the target market for our system. The Australian landscape's complex renewable infrastructure planning combined with numerous abandoned mine sites requires our machine learning model to accommodate users of differing technical skill levels and various goals.

The interface experience is tiered to support different user groups as per the solution design. Strategic decision-makers need only high-level summaries and rankings, but technical users gain access to filters and weightings for a more detailed custom analysis of raw outputs. The combination of these methods ensures that all stakeholders receive accessible, flexible solutions which deliver value.

2.1 Primary Users

2.1.1 Product Development Team
The Product Development Team identifies suitable locations for setting up gravity battery systems. The tool helps our evaluation process by producing a sorted list of promising mine sites which considers geographic location, geological aspects, and infrastructure parameters. The automation process provides time savings while eliminating the subjective elements of manual analysis.

2.1.2 Investment and Strategy Managers
Stakeholders require analytical data to inform their decisions about capital investments. Green Gravity's system provides predictive site scoring and ROI indicators which enable stakeholders to concentrate their efforts on opportunities that promise financial success while supporting their strategic goals.

2.2 Secondary Users

2.2.1 Engineering Consultants and Contractors
Engineering consultants and contractors use pre-assessed sites to perform feasibility studies and create engineering designs specific to each location. The tool enables this capability by exporting essential site parameters along with technical markers that indicate lifting infrastructure suitability and terrain access potential for construction planning.

2.2.2 Government and Regulatory Bodies
The system's high-confidence recommendations provide essential support for regulatory bodies focused on mine rehabilitation and energy transition planning as governments increasingly emphasise sustainability. Our system outputs provide justification for grants and fast-track approvals while helping to prioritise environmentally beneficial projects.

2.2.3 Research and Environmental Analysts
The model's datasets and visual outputs provide resources for academic modelling and policy planning to university researchers and third-party analysts. Deriving data layers for public use will enhance community involvement and foster new cross-sector developments.

===============================================================================

3. Design Methodology

3.1 Overview

Our Project follows the Agile Design methodology structured around 4-week sprints. Agile will suitably fit this project due to its adaptability and emphasis on iterative progress which will align with the evolving and exploratory nature of this ML model. This is particularly true when dealing with variable-quality public datasets and experimental modelling. Given the uncertainty around data availability, model accuracy and integration feasibility, agile allows us to rapidly test hypotheses, incorporate feedback and refine our pipeline with each sprint. This is crucial for us to manage complex tasks around geospatial mapping, machine learning and energy systems modelling.

3.2 Agile Methodology

Agile allows us to have an incremental development schedule, where features are delivered in small, testable components from data ingestion scripts to mock scoring models. It encourages us to focus on lean, living documents that evolve alongside the product. Our data framework, pipeline diagrams, and technical notes are updated iteratively and stored in version-controlled repositories, ensuring they reflect the current implementation rather than static, outdated plans.

Agile also influences our workflow through regular reviews, with sprint retrospectives and workshops used to refine goals and technical approaches. Adaptive planning is a key benefit — we plan tasks per sprint in Jira, enabling us to shift priorities in response to technical blockers or emerging data challenges.

Sprint Timeline:
Sprint 1 (Mar 24 – April 6): Project Planning and initial Setup
- Setup GitHub, Jira, team workspace
- Define basic scope and goals
- Define tiers of data

Sprint 2 (Apr 7 – Apr 20): Data Source Exploration
- Identify and Document data sources
- Download and manual exploration of key data sets
- Assign data to tiers

Sprint 3 (Apr 21 – May 4): Data Pipeline Testing
- Unify data formats
- Coordinate plotting
- Document cleaning
- Handling missing data values

Sprint 4 (May 5 – Jun 2): Full Dataset Cleaning & T1–T2 Structuring
- Clean all datasets systematically and automatically
- Tier 1 + 2 data extractions

Sprint 5 (Jun 3 – Jun 30): MVP Model: Scoring & CLI Ranking
- Basic evaluation functions
- Build CLI tool for mine ranking
- Test and validate outputs

Sprint 6 (Jul 1 – Jul 28): Grid Integration & Tier 3 Modelling
- Proximity to generators and loads
- Grid patterns and usage
- Energy demand fluctuations
- Integration into the ML model

Sprint 7 (Jul 29 – Aug 25): Tier 4–5: Finance & Investment Layer
- Integrate into the model:
  - Land cost
  - Site rehabilitation cost
  - Energy grid pricing
  - Long-term energy demand
  - Future renewable generators

Sprint 8 (Aug 26 – Sep 22): Pipeline Build & Basic Interface
- Build ingestion > model > final view pipeline
- UI Mockups

Sprint 9 (Sep 23 – Oct 24): Testing, Polishing & Final Delivery
- Final code review and cleanup
- Documentation and user guides
- Final model evaluation

3.3 Milestones

Data Ingestion Layer – Sprint 4 (Jun 2)
At this milestone we plan to solve the loading and standardising of raw data from local and online sources and prepare the data for ML modelling

Machine Learning Model – Sprint 5 (Jun 30)
At this milestone we plan to deliver a mock evaluation model to score and rank mineshafts and output a ranked list ordering the mineshafts from most to least favourable.

View Layer – Sprint 8 (Sep 22)
At this layer we plan to have a working command-line interface to display ranked results from the model to complete the Proof-of-concept pipeline, integrating all layers from the ingestion to presentation.

===============================================================================

4. Development Environment

4.1 Development Tools

The project is developed using Visual Studio Code as the primary IDE however, the project is not IDE specific. Version control is managed through Bitbucket which was chosen over GitHub for its familiarity. Commitizen is used to standardise commit messages and ensure a consistent commit history and version naming. To streamline development and testing, Make is used to automate common command line tasks. We also use the development tools, pytest, black, isort, flake8 and mypy to ensure formatting consistency and enable automated testing.

4.2 Programming Languages and Libraries

The project is developed in Python, chosen for its speed of development and prototyping in line with the Agile methodology used by our team. Python has a large ecosystem of open-source materials for the exact purposes of our project. The libraries Pandas and GeoPandas enable the handling of geospatial datasets which is key to the project, DEAP and TensorFlow enable the fast development of ML models which is a key criterion for the project.

All these libraries add the speed of development by removing key challenges allowing the team to focus on the overall development of the project, this speeds up prototyping aiding in the Agile workflow. We use MongoDB to store geospatial data layers, and the local development environment runs it within a Docker container. MongoDB was chosen for its prebuilt spatial joins, and Docker has been chosen for the local development environment for its simplicity of use and ability to simulate a production environment.

4.3 Collaboration and Communication Tools

Bitbucket is also used for code review of pull requests. Jira is used for task tracking, sprint planning, and was chosen for its integration with BitBucket. Communication among team members and with Green Gravity is conducted on Microsoft Teams. Microsoft Teams was chosen as it can host direct messages, group messages, video calls and organise documentation.

===============================================================================

5. Deployment Environment

5.1 Local Development and Testing

Currently our team uses localised hosting for development and testing. Our environment consists of a Python repository and Docker Desktop which houses our MongoDB persistent containers. For speed and flexibility purposes, local development is the industry standard and allows for timely debugging, testing and performance monitoring.

We leverage Makefiles in our repository to orchestrate interactions between containers and our application. While appearing simple, these execution files perform crucial functions: setting environment variables, running docker commands, managing dependencies, and executing Python scripts. This lightweight yet powerful approach eliminates manual configuration, automatically managing environments to simulate testing, deployment, and code monitoring without the need for complex build agent setup.

5.2 Cloud Deployment

Our production deployment strategy involves hosting the MongoDB database on Microsoft Azure while implementing an API interface using serverless functions. Implementation may include:
- Azure Functions for our lambda function implementation
- Azure App Service for our web application hosting
- Azure Cosmos DB for our NoSQL database solution
- Azure Key Vault for secure credential management
- Azure API Management for API gateway functionality

We would design a CI/CD pipeline to deploy a serverless backend and client (web-app) for our production system which communicates with a NoSQL database hosted using Cosmos. Secrets would be centrally managed by Key Vault and logs streamed to Datadog for monitoring and alerts.

===============================================================================

6. ETL Pipeline (MVP)

6.1 Business Context

Based on requirements set by our client, our task is to develop a Data Ingestion Layer that will extract, transform and load (ETL) data for our machine learning model. Our first deliverable to the client is a pipeline that can transform raw data and seed a NoSQL database with normalised geospatial data. This component will be used as a foundation for our 'Lucent v1' model, to be developed in the near future.

6.2 Solution Overview

A data ETL pipeline designed to support our machine learning model and visualisation interface.

6.3 Requirements

6.3.1 Functional
- Ingest multiple data formats (CSV, JSON, XML)
- Support both file-based and API-based data extraction
- Normalise raw data into geospatial data using GeoJSON
- Support spatial-aggregated queries

6.3.2 Non-Functional
- Be designed using best practices (OOP)
- Be scalable and extensible for future expansion
- Be designed in a maintainable and user-friendly fashion

6.4 Modules

Our MongoDB module manages geospatial data by connecting to MongoDB databases and providing specialised methods for saving GeoDataFrames as collections with geospatial indexing. It also implements spatial proximity analysis that can aggregate related documents across different collections based on their geographic locations, complete with detailed logging and summary statistics.

Our Pipeline module is designed for processing geospatial data from various file formats (CSV, JSON, GeoJSON, Excel) and APIs. It extracts the data, converts it to GeoDataFrames with proper coordinate handling, normalises and processes it, then outputs the results as a GeoJSON feature collection with detailed summaries of the aggregation process.

6.5 Future Development

Our goal is to test our Data Ingestion Layer with real datasets instead of dummy data. This will validate the ETL's robustness after our work on edge cases and data formatting.

===============================================================================

7. Tools

7.1 Development and Communication Tools

To ensure a successful delivery our ML model we have adopted a curated set of tools to support project management, design, development, testing and documentation. Each tool selected is based on its alignment with the team's workflow, familiarity and the ability to support an agile, collaborative workflow.

- Jira: Use to manage our agile workflow with sprint planning, task tracking and backlog prioritisation
- Microsoft Teams: Serves as our main communication platform among the team and weekly meetings with Green Gravity stakeholders throughout the project.
- Microsoft Word: Used as our formal report writing and submitting project deliverables in a professional format.
- SharePoint: Facilitates the secure data sharing between Green Gravity and our team for public and private datasets.
- Bitbucket: For hosting our codebase within Git repositories to ensure distributed version control for collaboration between developers.
- Docker: used for containerising the application ensuring that functionality remains consistent between development environments.

7.2 Software Tools

Our project uses Python as the primary programming language, chosen for its strength in data manipulation, machine learning ecosystems and alignment with the client's existing technology. We leverage core open-source libraries including NumPy and Pandas, which provide us with the computational foundation required for our data science operations.

Initially there was some consideration regarding SQL databases considering the tabular structure of DataFrames, however our normalised GeoJSON format works natively with MongoDB, hence our decision to use NoSQL was easy, for features like spatial computation and native formatting. The document-oriented approach of MongoDB perfectly accommodates our data requirements without forcing unnecessary transformations.

This technology combination creates an efficient workflow that maximises our ability to handle complex geospatial operations while maintaining compatibility with the client's systems.

===============================================================================

8. Requirements Analysis

Below are the requirements for the project gathered through a combination of weekly meetings with David and Matt, and our own groups online research. They are structured into 3 categories, Base requirements, Additional Requirements and Stretch Goals to reflect their criticality and priority in the project. The Base Requirements define the essential system capabilities needed to support initial feasibility studies, including data ingestion, geospatial analysis, and mineshaft evaluation. Additional Requirements capture enhancements that improve usability and expand functionality, while Stretch Goals describe ambitious future integrations such as digital twins and market platforms that could significantly elevate system performance and relevance over time.

8.1 Base Requirements

Requirement | Description | System goal for requirement
---|---|---
Data Ingestion Pipeline | Collect, clean, and standardise data from local and open-access sources | Provide reliable input for machine learning
Geospatial Analysis | Overlay of mining sites with environmental, zoning, and infrastructure maps | Allow informed site selection based on spatial context
Evaluate mineshaft distance to grid | Overlay mineshafts distance to grid structures (aemo) and future renewables | Ensure cost-effective placement for infrastructure
Mine Evaluation | Assess and rank decommissioned mineshafts | Support decision-making for gravity energy systems
Model Evaluation Layer | Implement mock scoring model for POC, expandable to real ML models | Enable MVP and later full-scale modelling
CLI output of results | Present ranked mine sites in a command line interface | Demonstration and testing of core functionality
Full ML Model | Incorporate grid integration, financial, and investment data into final scoring model | Final full system incorporating all requirements

8.2 Additional Requirements

Feature | Description | Priority
---|---|---
API for Data Sources | Enable automatic updates of datasets via APIs | High
Presentation of results into Web UI | Replace CLI with visual interface | Medium
Energy Dispatch Simulation | Simulate energy movement and dispatch from mine sites to the grid | Medium
Data Query and Filter Functions | Allow users to sort or filter mine data in the UI | Medium

8.3 Stretch Goals

Goal | Description | Feasibility
---|---|---
Geospatial Dashboard with Interactive Maps | Live map visualisation tool using QGIS or web GIS integration | Medium
Integration with Energy Market Platforms | Real-time connectivity to market trading systems for price prediction | Low
Economic Forecasting Model | Long-term financial modelling based on market trends and renewable energy forecasts | Low
Implementation into Digital Twin | Connect system data and analytics with a real-time digital twin model of energy infrastructure | Medium

===============================================================================

9. Data Classification Framework

Our team has designed a data framework that allows us to categorise the different parameters impacting our problem domain. This framework arose from the understanding that some data points couldn't be bound to one category (i.e. financial, geographical). This hierarchy progresses from tier 1 to 5, with each successive level building upon the insights gained from the previous tiers.

This framework provides a standardised methodology for categorising and prioritising parameters relevant to our model. As the project evolves, new data can be identified and grouped based off its semantic value, allowing for seamless integration into our existing model.

Data Categorisation Hierarchy (Tier 1-5, prioritised by business impact):

9.1 Technical Parameters (T1)

The first layer (T1) of data are technical parameters, quantitative measurements that characterise the physical attributes of the mine shaft. This data type provides foundational dimensional data that can be used to calculate the size and storage capacity of the gravity battery. These parameters are simple measurements and calculation which support the following "Does this mine shaft possess the requisite physical characteristics to support gravity battery implementation?". Possible data could include:
- Mine Shaft Dimensions
- Max Energy Storage

9.2 Site-Specific Conditions (T2)

The second layer (T2) of data are site-specific conditions, physical characteristics of the mine site which describe how difficult construction and rehabilitation will be. This data type provides insight into the condition of the mine site and how suitable it is to store a gravity battery. These parameters will require some preprocessing and abstraction to support the following question: "How challenging will construction and rehabilitation be at this site?" Possible data could include:
- Weather and climate
- Local site conditions
- Proximity to infrastructure
- Mineshaft collapse or shifting which is bad

9.3 Grid Integration (T3)

The third layer (T3) of data are grid integration parameters, which address how the energy storage system connects with and supports the broader electricity network. This data type focuses on energy movement and storage efficiency to answer the question: "How effectively can this site integrate with existing power infrastructure?" Possible data could include:
- Proximity to generators and loads
- Grid patterns and usage
- Energy demand fluctuations
- Algorithm that controls dispatch to the grid
- Grid size in proximity
- Energy loss over distance

9.4 Financial Analysis (T4)

The fourth layer (T4) of data are financial analysis metrics, which are concerned with the cost and return from the mine site. This data type provides purely financial metrics related to money to answer the question: "What are the financial costs and potential returns of this gravity battery project?" Possible data could include:
- Land cost
- Site rehabilitation cost
- ROI
- Energy grid pricing

9.5 Investment Analysis (T5)

The fifth layer (T5) of data are investment analysis parameters, which evaluate the long-term viability of the investment. This data type provides insights into prospects to answer the question: "Is this project viable as a long-term investment?" Possible data could include:
- Long-term energy demand
- Future renewable generators
- Risk assessment

===============================================================================

10. Data Integration Plan

The data collected from the above sources will be processed through the following steps to support both our MVP and full model. This plan is comprised of four steps, allowing us to collect, preprocess and analysis that data to be used within the model.

4-Step Data Integration Framework:

10.1 Data Cleaning and Normalisation

Data cleaning and normalization comprise the initial step towards achieving dataset accuracy and compatibility. We must first recognize and resolve discrepancies between field names together with measurement units, coordinate formats and data types. One dataset might record mine shaft depth using meters while another uses foot which demands unit standardization. Incomplete or missing data entries will undergo filtering or verification marking. Data preparation requires this step to successfully train machine learning models by establishing a stable base for analysis.

10.2 Geospatial Mapping and Overlay

The cleaned data will be transferred into geospatial software platforms like QGIS or ArcGIS which enables visual examination and spatial correlation analysis. The coordinates of each mineshaft will be plotted and combined with several contextual layers including climate zones, infrastructure networks, renewable generation sites, and land zoning regulations. The team will be able to visualise potential conversion sites by understanding how mineshaft attributes connect with external environmental components. Spatial analysis relies on these overlays because they enable decision-makers to access Information-dense visual representations.

10.3 Tiered Data Framework

The project's data framework specifies a five-tier classification model (T1 to T5) which will structure and process data. The initial model development and MVP will concentrate mainly on Tier 1 technical parameters and Tier 2 site-specific conditions because they are the most accessible data sets that directly inform physical feasibility.

When the project advances Tiers 3 through 5 which include grid integration and financial and investment analysis will be progressively incorporated to improve the model's sophistications and precision. The system evolves through minor development stages and enables immediate functionality together with the ability for later expansion.

10.4 Scalability and Extensibility

The project's future growth will be dependent by implementing a data storage architecture that scales well and extends easily for model refinement. The project will use structured databases for storing cleaned datasets which allows smooth integration with the machine learning model and future user interface. Users can perform real-time data queries and filtering and add new datasets when they become available through the database system.

The system will establish API connections to datasets whenever feasible to enable automatic updates and ongoing enhancement of model inputs without requiring manual actions. The innovative design approach guarantees that the platform will remain sustainable and adaptable over time.

Through its procurement approach the Green Gravity project gains access to quality Australian data at no cost which enhances the model's technical accuracy and scalability. Our methodology guarantees transparency and repeatability when we use open datasets which supports dedication to academic and commercial energy research practices.

===============================================================================

11. Design

11.1 Logo

Since our project is designed to transform data into valuable insights for our clients, we chose the name Lucent (from "lucens" in Latin, meaning "to shine"). This name perfectly demonstrates our goal of using machine learning to shine light on previously hidden patterns and connections within complex datasets, illuminating the path to better decision-making through data-driven insights.

The addition of 'v1' signifies the first iteration of our model, emphasising continuous improvement over time. We intend for our final product to supersede version one, instead being the result of multiple improvements over time.

11.2 Style Guide

Branding style guide includes typography, colors and UI design elements.

===============================================================================

12. Conclusion

Our team executed a comprehensive design and development process to create a machine learning-powered tool that assists Green Gravity in their renewable energy storage transformation efforts. Our Lucent v1 system uses decommissioned mineshaft and modern data analytics to automate site selections, which reduces human workload and improves decision-making precision.

Our first step was to recognise and organise user groups so that our solution could meet the requirement of the strategic decision-makers along with technical developers and external parties including regulators and researchers. Our iterative development cycle followed agile methodology which enables us to overcome data limitations quickly while effectively incorporating feedback through organised sprints and milestone tracking.

Our choice of development and deployment environments reflects best practices in data science and software engineering concepts. Utilising python alongside MongoDB and Docker combined with Azure cloud services creates a system with strong maintainability and future-readiness. The combination of our data classification framework and integration pipeline enables the system to handle more complicated datasets which vary from site-specific technical detail to extended financial forecast.

Our joint work achieved both ETL pipeline construction mine scoring requirement while enabling advanced development possibilities such as web-based interfaces and investments forecasting through digital integration. Lucent v1 delivers a robust and intelligent decisions-support tool to green gravity that enables data-driven, scalable deployment of gravity battery infrastructure thought out Australia.

===============================================================================

References

AEMO (2013). Australian Energy Market Operator. [online] Australian Energy Market Operator. Available at: https://www.aemo.com.au/.

Atlassian (2020). Bitbucket. [online] Bitbucket. Available at: https://bitbucket.org/.

Atlassian (2024). Jira. [online] Atlassian. Available at: https://www.atlassian.com/software/jira.

Docker (2024). Enterprise Application Container Platform | Docker. [online] Docker. Available at: https://www.docker.com/.

geojson.org. (n.d.). GeoJSON. [online] Available at: https://geojson.org/.

Microsoft (2020). SharePoint, Team Collaboration Software Tools. [online] www.microsoft.com. Available at: https://www.microsoft.com/en-us/microsoft-365/sharepoint/collaboration.

Microsoft (2024). Visual Studio Code. [online] Visualstudio.com. Available at: https://code.visualstudio.com/.

MongoDB (2024). The most popular database for modern apps. [online] MongoDB. Available at: https://www.mongodb.com/.

Pydata.org. (2018). Python Data Analysis Library. [online] Available at: https://pandas.pydata.org/.

PYTHON (n.d.). Python. [online] Python.org. Available at: https://www.python.org/.

QGIS (2025). Welcome to the QGIS project! [online] qgis.org. Available at: https://qgis.org/.

www.gnu.org. (n.d.). gnu.org. [online] Available at: https://www.gnu.org/software/make/.

www.microsoft.com. (n.d.). Microsoft Word - Word Processing Software | Office. [online] Available at: https://www.microsoft.com/en-us/microsoft-365/word.

www.microsoft.com. (n.d.). Video Conferencing, Meetings, Calling | Microsoft Teams. [online] Available at: https://www.microsoft.com/en-us/microsoft-teams/.

===============================================================================

Glossary

API (Application Programming Interface): A set of rules that allows different software applications to communicate with each other.

CI/CD (Continuous Integration/Continuous Deployment): Development practices that involve automatically integrating code changes and deploying them to production environments.

CLI (Command Line Interface): A text-based interface used to interact with software and operating systems through typed commands.

ETL (Extract, Transform, Load): A process that extracts data from various sources, transforms it into a suitable format, and loads it into a target database.

Machine Learning (ML): A subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.

MVP (Minimum Viable Product): A version of a product with just enough features to be usable by early customers who can then provide feedback for future development.

POC (Proof of Concept): A realization of a certain method or idea to demonstrate its feasibility or a demonstration in principle to verify that some concept or theory has practical potential.

ROI (Return on Investment): A performance measure used to evaluate the efficiency of an investment or compare the efficiency of several different investments.

UI (User Interface): The point of human-computer interaction and communication in a device.

SQL (Structured Query Language): A standard programming language designed for managing and manipulating relational databases, allowing for data retrieval, insertion, updating, and deletion through standardized commands.